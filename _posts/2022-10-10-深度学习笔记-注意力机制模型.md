---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记::注意力机制模型              # 标题 
subtitle:   Transformer、Conformer and 代码解读    #副标题
date:       2022-10-10              # 时间
author:     Aaron Mao               # 作者
header-img: img/mac-night.webp    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Deep Learning
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


> **Content：深度学习之注意力模型，主要讨论Transformer和Conformer**   
> **更新日志：2022.10.10 更新**

## 目录

*   [2.6 注意力机制](#26-注意力机制)

    *   [2.6.1 Transformer](#261-transformer)

        *   [参考资料](#参考资料)

        *   [Motivation](#motivation)

        *   [1、整体架构](#1整体架构)

        *   [2、Embedding和位置编码](#2embedding和位置编码)

        *   [3、Encoder](#3encoder)

        *   [4、Self-Attention](#4self-attention)

        *   [5、MHSA](#5mhsa)

        *   [6、Layer Norm](#6layer-norm)

        *   [7、Decoder](#7decoder)

        *   [8、Mask](#8mask)

        *   [8、输出层Generator](#8输出层generator)

        *   [9、训练和推理过程](#9训练和推理过程)

    *   [2.6.2 Transformer代码解读-哈佛版本](#262-transformer代码解读-哈佛版本)

        *   [Embeddings 和位置编码](#embeddings-和位置编码)

        *   [Encoder](#encoder)

        *   [Attention](#attention)

        *   [mask](#mask)

        *   [Decoder](#decoder)

        *   [输出](#输出)

        *   [模型架构](#模型架构)

    *   [2.6.3 Conformer](#263-conformer)

        *   [结构：**FF+MHSA+Conv+FF**](#结构ffmhsaconvff)

        *   [**深度可分卷积**](#深度可分卷积)

# 深度学习-注意力机制模型

> 💡顺着自己的理解，结合多篇博客整理了一下

## 2.6 注意力机制

### 2.6.1 Transformer

#### 参考资料

神经网络与深度学习 邱锡鹏 P199 P389-340

<https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer>

<https://blog.csdn.net/u012526436/article/details/86295971>

#### Motivation

*   RNN的计算是有方向顺序的，时间片t的计算依赖于上一个时刻t-1的结果，限制了模型并行能力，并且有梯度消失的问题（长程建模能力弱）

*   LSTM可以缓解一定的长期依赖，但是特别长的情况，依然不行

*   Transformer结构仅由self-attention 和 feed forward 神经网络 抛弃CNN和RNN， 具备更好的并行性，符合现有的GPU框架

*   attention 机制可以将序列中的任意两个位置距离缩小为常量

#### 1、整体架构

*   Input:

    *   Embedding + Pos Encoding

*   EncoderLayer：

    *   MHSA（padding mask）

    *   FFN

*   DecoderLayer:

    *   Masked MHSA（seq mask）

    *   Cross MHSA （padding mask）

    *   FFN

*   Generator：

    *   Linear + Softmax

![](https://itsMao.github.io/img/2022-10-08-image/image_pSjBYGtZYr.png)

![](https://itsMao.github.io/img/2022-10-08-image/image_1smSwQymyz.png)

> 💡先看模型的输入

#### 2、Embedding和位置编码

*   **Embedding**：
    将输入的one-hot序列转为长度为$d_{model}$（论文中为512， 后续图形示例为4）的embedds向量，通常有两个做法：

    *   预训练得到的Lookup table，然后乘以 $\sqrt{d_{model}}$，这点类似于语言模型，完成词表大小C到隐层H的的尺度转换

    *   直接end-to-end训练得到

*   **位置编码：**
    RNN当前点的和之前的点输入有关，因此对顺序敏感，对相对和绝对位置都敏感； &#x20;
    attention只能挖掘两个点的相关程度，不知道前后的区别，因此对位置不敏感；
    我们可以人为地给每个点加一个确定的位置信息；
    位置编码有正弦编码，余弦编码，也可以是learnable

    *   固定位置编码（奇数和偶数位置不同方式）：

    ![](https://itsMao.github.io/img/2022-10-08-image/image_6WjsP_-g8J.png)

    PE就是一个长度为pos的编码矩阵
    编码可以灵活推广到更长的测试句子，因为PE(pos+k)可以写成PE(pos)的线性组合

*   **最后，Embedding和位置编码相加**，得到一个句子（下文以两个单词为例）的表征向量x\[B, 2, 4]

#### 3、Encoder

单个EncoderLayer如下图所示：

![](https://itsMao.github.io/img/2022-10-08-image/image_BQPXKGzr-9.png)

Encoder中，假设输入只有两个单词x1 和 x2，它们 经过了某种信息交换和杂糅，得到了中间变量z1和z2，然后z1和z2各自通过FFN得到r1和r2

假设输入的句子$x=[x_1, x_2, ..., x_{14}]$，经过self-attention映射为$z=[z_1, z_2, ..., z_{14}]$，其实就是表征了当前单词和其他单词的相关程度，那**相关程度如何计算呢？** →注意力机制

#### 4、Self-Attention

计算机制如下：

![](https://itsMao.github.io/img/2022-10-08-image/image_zoix4IFy_e.png)

![](https://itsMao.github.io/img/2022-10-08-image/image_p2uNRhFs8o.png)

可以这样理解，z1就是单词x1作为query，和每一个单词（key）计算相似度，然后对其value（某种表征单词的隐层信息）进行加权组合，可以推测，关联程度更好的单词（比如x4）的信息就会流入到z1中，后续也可以通过z1恢复出关联程度最高的单词x4。

在具体计算方面，一个 $x[1, d_{model}]$变换出QKV就得做三次乘法
实际一个句子中会有n个x，堆叠形成矩阵 $x[n, d_{model}]$可以用矩阵操作进行加速，得到Attention公式：

$$
Attention(Q,K,V)=softmax \frac{QK^T}{\sqrt{d_k}}V
$$

这里**为什么用**$\sqrt{d_k}$**对QK进行scaled**呢？因为这是在规范化数据的分布，梯度敏感，好训练；加根号则是经验结果。

#### 5、MHSA

为了增强鲁棒性，可以用不同的矩阵去线性变换X到QKV，在不同的投影空间中去学习相似关系，比如学习8个z向量，然后拼接起来，再通过全连接网络（权重矩阵）恢复到x相同维度

![](https://itsMao.github.io/img/2022-10-08-image/image_mjGIOCk_27.png)

#### 6、Layer Norm

每一个子层用一个残差结构和一个layerNorm， 残差结构不做解释，&#x20;
LayerNorm在每一个样本上计算均值方差，一层神经元一起归一化，
原文是使用了post-LN，也有代码实现使用了pre-LN，孰优孰劣就不知道了，
顺便一提，BatchNorm是在batch维度上归一化，一个batch中的相同神经元进行归一化

#### 7、Decoder

DecoderLayer和EncoderLayer不一样，第一层是Masked MHSA，第二层是Cross MHSA，然后是一个FFN。
Masked的原因我们后面再看，Cross这里，attention输入Q是上一层的输出，而KV是encoder的输出，在代码中称为memory，可以理解Encoder已经从词表中学习了一套KV数据库了，我们输入任何一个Q，就能根据K查找，判断一个映射输出。

![](https://itsMao.github.io/img/2022-10-08-image/image_Kq84NsYW8g.png)

#### 8、Mask

Transformer里面有两种mask：padding mask and sequence mask

*   \*\*padding mask: \*\*
    一个boolean张量，保持输入序列长度对齐，较短的补0，较长的舍弃多余，
    具体做法：用在任何一个MHSA中，计算Softmax之前，把padding位置加上非常小的负数-1e9，softmax结果趋近于0，从而使得attention不会在这里投入注意力

    ![](https://itsMao.github.io/img/2022-10-08-image/image_Cf8SVasPkh.png)

    ![](https://itsMao.github.io/img/2022-10-08-image/image_KUzYjm3d14.png)

*   **Sequence mask（非常重要）:**
    使decoder不看未来信息，把当前时刻t后面的部分隐藏起来
    具体做法：用在masked MHSA 中，会提前产生一个上三角矩阵（对角线上方为1），QK相乘后，矩阵对应位置mask为1的部分，值设置为-1e9，softmax结果趋近于0，相当于忽略了未来信息

    ![](https://itsMao.github.io/img/2022-10-08-image/image_2lHy9XBVaq.png)

#### 8、输出层Generator

decoder最后输出一个r向量，表征了当前这个单词，最相关单词的信息；
用全连接层变换到词典维度（假设1w个词），用softmax生成1w个概率；
最后输出概率最大的词

#### 9、训练和推理过程

<https://blog.csdn.net/qq_42599237/article/details/123383691>

*   **推理：**

    第一步：

    *   Encoder输入：“我爱中国” embedding

    *   Decoder输入： `<\s> `

    *   Decoder输出：`I`（预测）

    第二步：

    *   Encoder输入：“我爱中国” embedding

    *   Decoder输入： `<\s> ``I `

    *   Decoder输出：`Love`（预测）

    第三步：

    *   Encoder输入：“我爱中国” embedding

    *   Decoder输入： `<\s> ``I`` ``Love`

    *   Decoder输出：`China`（预测）

    ...

*   **训练：**

    *   Encoder输入：“我爱中国” embedding

    *   Decoder输入： `<\s> I Love China `

    *   Decoder输出预测：`I Love Anhui <\EOF>`

    *   Decoder输出标签：`I Love China <\EOF>`&#x20;

    输出标签是输入的右移

### 2.6.2 Transformer代码解读-哈佛版本

<https://nlp.seas.harvard.edu/2018/04/03/attention.html>

<https://cloud.tencent.com/developer/article/1885525>

#### Embeddings 和位置编码

```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        """
        d_model：embedding的维度
        vocab: 词表的大小
        """
        super(Embeddings, self).__init__()
        #lookuop table, 直接获取one-hot向量对应的嵌入表示
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model =d_model
    def forward(self, x):
        """
        参数x：单词文本通过词表映射后的one-hot向量
        """
        embedds = self.lut(x)
        return embedds * math.sqrt(self.d_model) # 变换到隐层大小
        
# Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        """
        d_model：词嵌入维度
        dropout: dropout触发比率
        max_len：每个句子的最大长度
        """
        super(PositionalEncoding, self).__init__() 
        self.dropout = nn.Dropout(p=dropout)
        # Compute the positional encodings
        # 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。
        # 这样计算是为了避免中间的数值计算结果超出float的范围，
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0., max_len).unsqueeze(1) # 增加维度
        div_term = torch.exp(torch.arange(0., d_model, 2) *
                             -(math.log(10000.0) / d_model)) #相对位置公式
         
        pe[:, 0::2] = torch.sin(position * div_term)   #取奇数列
        pe[:, 1::2] = torch.cos(position * div_term)   #取偶数列
        pe = pe.unsqueeze(0)           # 增加维度
        self.register_buffer('pe', pe)
         
    def forward(self, x):
        # embedding 与positional enco相加
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) 
        return self.dropout(x)
```

#### Encoder

```python
class Encoder(nn.Module):
    "Core encoder is a stack of N layers"

    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N) # EncoderLayer
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask) #把序列跟mask输入到每层中
        return self.norm(x)
        
# 单个编码器layer       
class EncoderLayer(nn.Module):
    "Encoder is made up of self-attn and feed forward (defined below)"
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        # 注意力子层
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        # FFN子层
        z = self.sublayer[1](x, self.feed_forward)
        return z
        
# 残差连接 和 preNorm （论文中是post-norm，这里不太一样）
class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x))) # pre-LayerNorm
        # return self.norm(x + self.dropout(sublayer(x))) # post-LayerNorm
```

#### Attention

```python
def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    # QKV shape: [h, x, d_k]
    
    # 取词嵌入大小
    d_k = query.size(-1) 
    # 注意力公式 QK^T/sqrt(d_k), QK相乘之后shape为[h, x, x]
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    # seq mask： 这里要搞清楚，subsequent_mask生成的是下三角阵，对角线上方为0
    if mask is not None:
        # 0的位置取非常小值-1e9，softmax结果会趋于0，不会引入注意力
        scores = scores.masked_fill(mask == 0, -1e9) 
    # 获得注意力张量
    p_attn = F.softmax(scores, dim = -1) # [h, x, x]
    if dropout is not None:
        p_attn = dropout(p_attn)
    # 返回query的注意力表征[h, x, d_k] 和 注意力张量[h, x, x]
    return torch.matmul(p_attn, value), p_attn
    
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model => [h, x, d_k] 
        # 将 QKV三个矩阵，各自经过全连接层，reshape[h, x, d_k]
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        # contiguous()断掉拷贝联系, 恢复形状，经过全连接层，返回
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
```

#### mask

```python
def subsequent_mask(size):
    # tgt_mask
    # 生成向后遮掩的掩码张量，参数size是掩码张量最后两个维度的大小，它最后两维形成一个方阵
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    
    # 然后使用np.ones方法向这个形状中添加1元素，形成上三角阵
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')

    # 最后将numpy类型转化为torch中的tensor，内部做一个1- 的操作。
    # 这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减。
    # 如果是0，subsequent_mask中的该位置由0变成1
    # 如果是1，subsequect_mask中的该位置由1变成0
    # 返回的还是一个下三角矩阵，对角线下方为1
    return torch.from_numpy(subsequent_mask) == 0

def padding_mask(sentence, pad_idx):
    mask = (sentence != pad_idx).int().unsqueeze(-2)  # [B, 1, L]
    return mask

def test():
    # 以最简化的形式测试Transformer的两种mask
    sentence = torch.LongTensor([[1, 2, 5, 8, 3, 0]])  # batch_size=1, seq_len=3，padding_idx=0
    embedding = torch.nn.Embedding(num_embeddings=50000, embedding_dim=300, padding_idx=0)
    query = embedding(sentence)
    key = embedding(sentence)

    scores = torch.matmul(query, key.transpose(-2, -1))
    print("\nscores = \n", scores)

    mask_p = padding_mask(sentence, 0)
    mask_s = subsequent_mask(sentence)
    print("mask_p = \n", mask_p)
    print("mask_s = \n", mask_s)

    mask_encoder = mask_p
    mask_decoder = mask_p & mask_s  # 结合 padding mask 和 Subsequent mask
    print("mask_encoder = \n", mask_encoder)
    print("mask_decoder = \n", mask_decoder)

    scores_encoder = scores.masked_fill(mask_encoder == 0, -1e9)  # 对于scores，在mask==0的位置填充-1e9
    scores_decoder = scores.masked_fill(mask_decoder == 0, -1e9)  # 对于scores，在mask==0的位置填充-1e9

    print("scores_encoder = \n", scores_encoder)
    print("scores_decoder = \n", scores_decoder)

```

#### Decoder

```python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
        
#使用DecoderLayer的类实现解码器层
class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        #初始化函数的参数有5个，
        #第一个是size，代表词嵌入的维度大小，同时也代表解码器的尺寸，
        #第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，
        #第三个是src_attn, 多头注意力对象，这里Q!=K=V，
        #第四个是前馈全连接层对象，最后就是dropout置0比率
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        #按照结构图使用clones函数克隆三个子层连接对象
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
 
    def forward(self, x, memory, src_mask, tgt_mask):
        #forward函数中的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量memory，
        #以及源数据掩码张量和目标数据掩码张量，将memory表示成m之后方便使用。
        "Follow Figure 1 (right) for connections."
        m = memory
        #将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，
        #因为是自注意力机制，所以Q,K,V都是x，最后一个参数是目标数据掩码张量，
        #这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据。
        #比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，
        #但是我们不希望在生成第一个字符时模型能利用这个信息，
        #因此我们会将其遮掩，同样生成第二个字符或词汇时，模型只能使用第一个字符或词汇信息，
        #第二个字符以及之后的信息都不允许被模型使用。
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        
        #接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k,v是编码层输出memory，
        #同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄露，
        #而是遮蔽掉对结果没有意义的padding
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        
        #最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果，这就是我们的解码器结构
        return self.sublayer[2](x, self.feed_forward) 

```

#### 输出

```python
class Generator(nn.Module):
    "Define standard linear + softmax generation step."
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)
```

#### 模型架构

```python
# Model Architecture
#使用EncoderDecoder类来实现编码器-解码器结构
class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. 
    Base for this and many other models.
    """
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        #初始化函数中有5个参数，分别是编码器对象，解码器对象,源数据嵌入函数，目标数据嵌入函数，以及输出部分的类别生成器对象.
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed    # input embedding module(input embedding + positional encode)
        self.tgt_embed = tgt_embed    # ouput embedding module
        self.generator = generator    # output generation module
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        #在forward函数中，有四个参数，source代表源数据，target代表目标数据,source_mask和target_mask代表对应的掩码张量,在函数中，将source source_mask传入编码函数，得到结果后与source_mask target 和target_mask一同传给解码函数
        memory = self.encode(src, src_mask)
        res = self.decode(memory, src_mask, tgt, tgt_mask)
        return res
    
    def encode(self, src, src_mask):
        #编码函数，以source和source_mask为参数,使用src_embed对source做处理，然后和source_mask一起传给self.encoder
        src_embedds = self.src_embed(src)
        return self.encoder(src_embedds, src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        #解码函数，以memory即编码器的输出，source_mask target target_mask为参数,使用tgt_embed对target做处理，然后和source_mask,target_mask,memory一起传给self.decoder
        target_embedds = self.tgt_embed(tgt)
        return self.decoder(target_embedds, memory, src_mask, tgt_mask)


# Full Model
def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    """
    构建模型
    params:
        src_vocab:
        tgt_vocab:
        N: 编码器和解码器堆叠基础模块的个数
        d_model: 模型中embedding的size，默认512
        d_ff: FeedForward Layer层中embedding的size，默认2048
        h: MultiHeadAttention中多头的个数，必须被d_model整除
        dropout:
    """
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))
    
    # This was important from their code. 
    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model
```

### 2.6.3 Conformer

CNN提取局部特征；
transformer建模长时间全局关系；
Conformer想把二者结合起来，做法就是在transformer编码结构中加一层卷积，然后把FFN拆成了三明治结构

#### 结构：**FF+MHSA+Conv+FF**

![](https://itsMao.github.io/img/2022-10-08-image/image_XRQz71dpZd.png)

![](https://itsMao.github.io/img/2022-10-08-image/image_m5zKT6zat0.png)

#### **深度可分卷积**

**可以减小**三分之一**参数量**，但是性能差一点，因为
**深度卷积，没有用到通道间的信息；
逐点卷积，只是为了恢复形状，缺乏空间信息**

维度变换过程：
attention→(B, C, N)
pointwise →(B, C\*2, N)
GLU→(B, C, N)
depthwise→(B, C, (N+2P-1)/S+1)=(B, C, N) **same padding**
pointwise→(B, C, N)

*   **常规卷积：**

    ![](https://itsMao.github.io/img/2022-10-08-image/image_BzIK0sI7vZ.png)

*   **Depthwise**，一个通道一个kernel
    没有利用不同通道在相同空间位置上的特征信息

    ![](https://itsMao.github.io/img/2022-10-08-image/image_ScjGtTeKVe.png)

*   **Pointwise**，
    变换通道，没有利用同一个通道的不同空间位置上的特征信息

    ![](https://itsMao.github.io/img/2022-10-08-image/image_pNkEFMSVro.png)
